{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5d17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the root directory of the repository\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..')) # Assuming your notebook is in a subdirectory\n",
    "\n",
    "# Add the root directory to Python's search path\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "# Now you can import the function from encoding.py\n",
    "from encoding import prefix_bin\n",
    "\n",
    "# ... rest of your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed78ad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sliding_window' from 'c:\\\\Users\\\\rexbr\\\\Documents\\\\Universität\\\\Master\\\\PM\\\\online_ppm_stability_HU-PM\\\\sliding_window.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import stream,tree,metrics\n",
    "import utils\n",
    "from encoding import prefix_bin\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os,json\n",
    "import datetime\n",
    "from collections import deque\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import utils\n",
    "import sliding_window\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime, time\n",
    "import importlib\n",
    "importlib.reload(sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f755e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, interval=2000):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(streaming_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1fff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(new_case, test_cases):\n",
    "    test_cases.append(new_case)\n",
    "    if len(test_cases) > test_size:\n",
    "        test_cases.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b13a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rexbr\\Documents\\Universität\\Master\\PM\\online_ppm_stability_HU-PM\\expereiment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458d7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bpic15': {'key_pair': {}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 45}, 'bpic17': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Start Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 15}, 'iro5k': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity'], 'maximum_prefix': 12}, 'synthetic_log_b': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity'], 'maximum_prefix': 12}, 'synthetic_log_bc1': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity'], 'maximum_prefix': 12}, 'synthetic_log_bc1c2': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity'], 'maximum_prefix': 12}, 'synthetic_log_bc2': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity'], 'maximum_prefix': 12}, 'road_traffic_fine_process': {'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 6}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Correct path to the JSON file\n",
    "# CHANGE HERE TO YOUR PATH\n",
    "file_path = r'C:\\Users\\rexbr\\Documents\\Universität\\Master\\PM\\online_ppm_stability_HU-PM\\dataset_parameters.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34ecd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Start Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 15}\n",
      "15\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_true_encoded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_true_encoded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_true_encoded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_true_encoded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_true_encoded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_pred_prob [{'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}, {'False': 0.9405186, 'True': 0.059481405}]\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_true_encoded [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "y_pred_prob [{'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}, {'False': 0.9470462, 'True': 0.0529538}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Before test and training streaming event predictive monitoring, grace period is preceded to initialize model by prefix length\n",
    "and obtain feature matrix to transform future events\n",
    "'''\n",
    "\n",
    "# Experiment settings\n",
    "dataset_label = 'bpic17'\n",
    "true_to_1 = {'True': 1, 'False': 0}\n",
    "\n",
    "# Invoke parameters for dataset\n",
    "with open(file_path,'r') as json_file:\n",
    "    parameters = json.load(json_file)[dataset_label]\n",
    "    print(parameters)\n",
    "    key_pair = parameters['key_pair']\n",
    "    catatars = parameters['categorical_attrs']\n",
    "    maximum_prefixs = parameters['maximum_prefix']\n",
    "\n",
    "for maximum_prefix in range(1, maximum_prefixs+1):\n",
    "    print(maximum_prefix)\n",
    "    performance_measure_type = 'ROCAUC'\n",
    "\n",
    "    dataset_loc = r'C:\\Users\\rexbr\\Documents\\Universität\\Master\\PM\\online_ppm_stability_HU-PM\\DATA\\logs' + '\\\\'+ dataset_label +'.csv'\n",
    "    try:\n",
    "        os.makedirs('./result/%s'%(dataset_label))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Set streaming classifier\n",
    "    classifier = 'xgb'\n",
    "    if classifier == 'xgb':\n",
    "        streaming_classifier = xgb.XGBClassifier(n_estimators = 100, learning_rate=0.01, verbosity =0)\n",
    "\n",
    "    streaming_db ={}\n",
    "    training_models ={}\n",
    "    test_cases = deque()\n",
    "    feature_matrix ={}\n",
    "\n",
    "    casecount = 0\n",
    "    rowcounter = 0\n",
    "    running_case = 0\n",
    "    prediction_result = dict()\n",
    "    for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "    finished_caseid = set()\n",
    "    \n",
    "    # Sliding window for training setting\n",
    "    window_size = 100\n",
    "    test_size = 30\n",
    "    training_windows = sliding_window.training_window(window_size,test_size)\n",
    "    training_models['prefix_%s'%(maximum_prefix)] = [streaming_classifier, 0]\n",
    "    \n",
    "    dataset = stream.iter_csv(\n",
    "            dataset_loc\n",
    "            )\n",
    "    totallength = len(list(dataset))\n",
    "\n",
    "\n",
    "    dataset = stream.iter_csv(\n",
    "                dataset_loc,\n",
    "                target='outcome'\n",
    "                )\n",
    "    enctype = 'Index-base'\n",
    "    \n",
    "    cdhappend ={}\n",
    "    for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "\n",
    "    for x,y in dataset:\n",
    "        rowcounter +=1\n",
    "        # Event stream change dictionary keys\n",
    "        x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "        if dataset_label !='bpic15':\n",
    "            x['ts'] = x['ts'][:-4]\n",
    "\n",
    "        x['outcome'] =y \n",
    "        # Initialize case by prefix length\n",
    "        caseid = x['caseid']\n",
    "        outcome = x['outcome']\n",
    "    #     progress = x['progress']\n",
    "\n",
    "        x.pop('caseid')\n",
    "        x.pop('outcome')\n",
    "\n",
    "    #     x.pop('progress')\n",
    "\n",
    "        case_bin = prefix_bin(caseid, x)\n",
    "        case_bin.set_enctype(enctype)\n",
    "\n",
    "        if caseid not in list(streaming_db.keys()):\n",
    "            case_bin.set_prefix_length(1)    \n",
    "            streaming_db[caseid] = []\n",
    "        elif caseid in finished_caseid:\n",
    "            pass\n",
    "        else:\n",
    "            case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "            case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "        # Encode event and cases and add to DB\n",
    "        case_bin.update_truelabel(outcome)   \n",
    "        case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "        ts = case_bin.event['ts']\n",
    "        streaming_db[caseid].append(case_bin)\n",
    "        # Detect label appeared case \n",
    "        if outcome != '' and caseid not in finished_caseid:\n",
    "            for i in streaming_db[caseid]:\n",
    "                i.update_truelabel(outcome)\n",
    "            finished_caseid.add(caseid)\n",
    "            # Adding newly finished case to training set.    \n",
    "            casecount +=1\n",
    "            # Grace period to collect feature matrix\n",
    "            case_length = len(streaming_db[caseid])\n",
    "            if case_length >= maximum_prefix:\n",
    "                if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                    feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "\n",
    "                case_length = len(streaming_db[caseid])\n",
    "                if case_length >= maximum_prefix:\n",
    "                    x = streaming_db[caseid][maximum_prefix-1]\n",
    "                    training_windows.update_window(x)\n",
    "                    update_test_cases(streaming_db[caseid], test_cases)\n",
    "\n",
    "                    if training_windows.retraining_count == test_size:\n",
    "                        x_training = pd.DataFrame.from_dict([i.encoded for i in training_windows.container])\n",
    "                        for i in x_training.columns.values: x_training[i] = x_training[i].fillna(0)\n",
    "                        feature_matrix['prefix_%s'%(maximum_prefix)] = x_training.columns.values\n",
    "                        y_training = [i.true_label for i in training_windows.container]\n",
    "                        # print(y_training)\n",
    "                        y_training_encoded = le.fit_transform(y_training)\n",
    "                        # print(y_training_encoded)\n",
    "                        training_models['prefix_%s'%(maximum_prefix)][0].fit(x_training, y_training_encoded)\n",
    "                        training_models['prefix_%s'%(maximum_prefix)][1] +=1\n",
    "                        training_windows.reset_retraining_count()\n",
    "\n",
    "                        '''\n",
    "                        Evaluate current model with test cases\n",
    "                        '''\n",
    "                        model_update_count = training_models['prefix_%s'%(maximum_prefix)][1]\n",
    "                        prediction_result[maximum_prefix][model_update_count] = {}\n",
    "                        y_truelist = []\n",
    "                        y_predlist = []\n",
    "                        for case in test_cases:\n",
    "                            if len(case) >= maximum_prefix:\n",
    "                                x = case[maximum_prefix-1]\n",
    "                                model = training_models['prefix_%s'%(x.prefix_length)][0]\n",
    "                                current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                current_event = pd.Series(current_event).to_frame().T\n",
    "                                y_pred = training_models['prefix_%s'%(x.prefix_length)][0].predict_proba(current_event)\n",
    "                                y_truelist.append(x.true_label)\n",
    "                                y_predlist.append(y_pred)\n",
    "\n",
    "                        prediction_result[maximum_prefix][model_update_count]['y_true'] = y_truelist\n",
    "                        prediction_result[maximum_prefix][model_update_count]['y_pred'] = y_predlist\n",
    "                        if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "                            cdhappend[maximum_prefix] = model_update_count\n",
    "\n",
    "    # print(\"y truelist\", y_truelist, \"length\", len(y_truelist))\n",
    "    # print(\"y predlist\", y_predlist, \"length\", len(y_predlist))\n",
    "    for evaluation_method in ['Accuracy', 'F1', 'ROCAUC']:\n",
    "        prefix = maximum_prefix\n",
    "        update_count =[]\n",
    "        evaluation_list = []\n",
    "        length_list = []\n",
    "        for update in prediction_result[prefix].keys():\n",
    "            y_true = prediction_result[prefix][update]['y_true']\n",
    "            y_pred_prob_ = prediction_result[prefix][update]['y_pred']\n",
    "            prediction_class = training_models['prefix_%s'%(maximum_prefix)][0].classes_\n",
    "            if len(prediction_class) == 1: prediction_class = ['False', 'True']\n",
    "            y_pred_prob = []\n",
    "            for i in y_pred_prob_:\n",
    "                # print(\"i:\", i)\n",
    "                y_pred_prob.append({prediction_class[0]: i[0][0], prediction_class[1]: i[0][1]})\n",
    "            # print(\"y_pred_prob\", y_pred_prob)\n",
    "            y_pred = [max(x, key=x.get) for x in y_pred_prob]\n",
    "            print(\"y_pred\", y_pred)\n",
    "            print(\"y_true\", y_true)\n",
    "            if maximum_prefix == 15:\n",
    "                # for some reason, the labels in y_pred are \"True\" and \"False\" instead of 1 and 0, while all the previous\n",
    "                # predictions are 1 and 0. This is a fix to make the labels consistent\n",
    "                y_pred = [true_to_1[y] for y in y_pred]\n",
    "                print(\"y_pred\", y_pred)\n",
    "            # Fit and transform y_true and y_pred\n",
    "            y_true_encoded = le.fit_transform(y_true)\n",
    "            # Ensure y_pred is a numpy array of strings\n",
    "            # Filter out unseen labels\n",
    "            seen_labels = set(le.classes_)\n",
    "            y_pred_filtered = [label for label in y_pred if label in seen_labels]\n",
    "\n",
    "            print(\"y_true_encoded\", y_true_encoded)\n",
    "            # print(\"y_pred_encoded\", y_pred_encoded)\n",
    "            update_count.append(update)\n",
    "\n",
    "            if evaluation_method =='Accuracy':\n",
    "                value = accuracy_score(y_true_encoded, y_pred)\n",
    "            elif evaluation_method =='F1':\n",
    "                value = f1_score(y_true_encoded, y_pred, average='macro')\n",
    "                # warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            elif evaluation_method =='ROCAUC':\n",
    "                print(\"y_pred_prob\", y_pred_prob)\n",
    "                if maximum_prefix == 15:\n",
    "                    # for prefix 15, the y_pred_prob is a list of string values again, so we need to access the values with 'True' and 'False' again\n",
    "                    y_pred_prob = np.array([[x['False'], x['True']] for x in y_pred_prob])\n",
    "                else:\n",
    "                    y_pred_prob = np.array([[x[0], x[1]] for x in y_pred_prob]) # np.array([[x['False'], x['True']] for x in y_pred_prob])\n",
    "                # warnings.filterwarnings(\"ignore\")\n",
    "                try:\n",
    "                    value = roc_auc_score(y_true, y_pred_prob[:, 1])\n",
    "                except:\n",
    "                    value = 0\n",
    "            evaluation_list.append(value)\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.plot(update_count, evaluation_list, label = 'Accuracy')\n",
    "        #     plt.plot(update_count, length_list)\n",
    "        if dataset_label == 'bpic15' or dataset_label == 'bpic17':\n",
    "            pass\n",
    "        else:\n",
    "            plt.axvline(cdhappend[prefix], color = 'black', linestyle ='--', label = 'Concept drift')\n",
    "        plt.title(prefix)\n",
    "        plt.xlabel('Model update count')\n",
    "        plt.legend(bbox_to_anchor=(0.65, -0.1), ncol=2)\n",
    "        plt.ylim(0,1)\n",
    "\n",
    "        if os.path.exists('./img/%s/%s/%s'%(dataset_label, classifier, 'Model updates')) == False:\n",
    "            os.makedirs('./img/%s/%s/%s'%(dataset_label, classifier, 'Model updates'))\n",
    "        if os.path.exists('./result/%s/%s/%s'%(dataset_label, classifier, 'Model updates')) == False:\n",
    "            os.makedirs('./result/%s/%s/%s'%(dataset_label, classifier, 'Model updates'))\n",
    "\n",
    "\n",
    "        with open('./result/%s/%s/Model updates/prefix_%s_%s update.pkl'%(dataset_label, classifier, prefix, evaluation_method), 'wb') as f:\n",
    "            pkl.dump(prediction_result, f)\n",
    "        plt.savefig('./img/%s/%s/Model updates/prefix_%s_%s update.png'%(dataset_label, classifier, prefix, evaluation_method), dpi=250)\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a01db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Start Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 15}\n",
      "15\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_pred ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n",
      "y_true ['False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Before test and training streaming event predictive monitoring, grace period is preceded to initialize model by prefix length\n",
    "and obtain feature matrix to transform future events\n",
    "'''\n",
    "\n",
    "# Experiment settings\n",
    "dataset_label = 'bpic17'\n",
    "\n",
    "# Invoke parameters for dataset\n",
    "with open(file_path,'r') as json_file:\n",
    "    parameters = json.load(json_file)[dataset_label]\n",
    "    print(parameters)\n",
    "    key_pair = parameters['key_pair']\n",
    "    catatars = parameters['categorical_attrs']\n",
    "    maximum_prefixs = parameters['maximum_prefix']\n",
    "\n",
    "for maximum_prefix in range(1, maximum_prefixs+1):\n",
    "    print(maximum_prefix)\n",
    "    performance_measure_type = 'Accuracy'\n",
    "\n",
    "    dataset_loc = r'C:\\Users\\rexbr\\Documents\\Universität\\Master\\PM\\online_ppm_stability_HU-PM\\DATA\\logs' + '\\\\'+ dataset_label +'.csv'\n",
    "    # dataset_loc = './DATA/logs/'+ dataset_label +'.csv'\n",
    "    try:\n",
    "        os.makedirs('./result/%s'%(dataset_label))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Set streaming classifier\n",
    "    classifier = 'xgb'\n",
    "    if classifier == 'xgb':\n",
    "        streaming_classifier = xgb.XGBClassifier(n_estimators = 100, learning_rate=0.01, verbosity =0)\n",
    "\n",
    "    streaming_db ={}\n",
    "    training_models ={}\n",
    "    test_cases = deque()\n",
    "    feature_matrix ={}\n",
    "\n",
    "    casecount = 0\n",
    "    rowcounter = 0\n",
    "    running_case = 0\n",
    "    prediction_result = dict()\n",
    "    for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "    finished_caseid = set()\n",
    "    \n",
    "    # Sliding window for training setting\n",
    "    window_size = 100\n",
    "    test_size = 30\n",
    "    training_windows = sliding_window.training_window(window_size,test_size)\n",
    "    training_models['prefix_%s'%(maximum_prefix)] = [streaming_classifier, 0]\n",
    "    \n",
    "    dataset = stream.iter_csv(\n",
    "            dataset_loc\n",
    "            )\n",
    "    totallength = len(list(dataset))\n",
    "\n",
    "\n",
    "    dataset = stream.iter_csv(\n",
    "                dataset_loc,\n",
    "                target='outcome'\n",
    "                )\n",
    "    enctype = 'Index-base'\n",
    "    \n",
    "    cdhappend ={}\n",
    "    for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "\n",
    "    for x,y in dataset:\n",
    "        rowcounter +=1\n",
    "        # Event stream change dictionary keys\n",
    "        x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "        if dataset_label !='bpic15':\n",
    "            x['ts'] = x['ts'][:-4]\n",
    "\n",
    "        x['outcome'] =y \n",
    "        # Initialize case by prefix length\n",
    "        caseid = x['caseid']\n",
    "        outcome = x['outcome']\n",
    "    #     progress = x['progress']\n",
    "\n",
    "        x.pop('caseid')\n",
    "        x.pop('outcome')\n",
    "\n",
    "    #     x.pop('progress')\n",
    "\n",
    "        case_bin = prefix_bin(caseid, x)\n",
    "        case_bin.set_enctype(enctype)\n",
    "\n",
    "        if caseid not in list(streaming_db.keys()):\n",
    "            case_bin.set_prefix_length(1)    \n",
    "            streaming_db[caseid] = []\n",
    "        elif caseid in finished_caseid:\n",
    "            pass\n",
    "        else:\n",
    "            case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "            case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "        # Encode event and cases and add to DB\n",
    "        case_bin.update_truelabel(outcome)   \n",
    "        case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "        ts = case_bin.event['ts']\n",
    "        streaming_db[caseid].append(case_bin)\n",
    "        # Detect label appeared case \n",
    "        if outcome != '' and caseid not in finished_caseid:\n",
    "            for i in streaming_db[caseid]:\n",
    "                i.update_truelabel(outcome)\n",
    "            finished_caseid.add(caseid)\n",
    "            # Adding newly finished case to training set.    \n",
    "            casecount +=1\n",
    "            # Grace period to collect feature matrix\n",
    "            case_length = len(streaming_db[caseid])\n",
    "            if case_length >= maximum_prefix:\n",
    "                if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                    feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "\n",
    "                case_length = len(streaming_db[caseid])\n",
    "                if case_length >= maximum_prefix:\n",
    "                    x = streaming_db[caseid][maximum_prefix-1]\n",
    "                    training_windows.update_window(x)\n",
    "                    update_test_cases(streaming_db[caseid], test_cases)\n",
    "\n",
    "                    if training_windows.retraining_count == test_size:\n",
    "                        x_training = pd.DataFrame.from_dict([i.encoded for i in training_windows.container])\n",
    "                        for i in x_training.columns.values: x_training[i] = x_training[i].fillna(0)\n",
    "                        feature_matrix['prefix_%s'%(maximum_prefix)] = x_training.columns.values\n",
    "                        y_training = [i.true_label for i in training_windows.container]\n",
    "                        y_training_encoded = le.fit_transform(y_training)\n",
    "                        # training_models['prefix_%s'%(maximum_prefix)][0].fit(x_training, y_training)\n",
    "                        training_models['prefix_%s'%(maximum_prefix)][0].fit(x_training, y_training_encoded)\n",
    "                        training_models['prefix_%s'%(maximum_prefix)][1] +=1\n",
    "                        training_windows.reset_retraining_count()\n",
    "\n",
    "                        '''\n",
    "                        Evaluate current model with test cases\n",
    "                        '''\n",
    "                        prediction_result[maximum_prefix][casecount] = {}\n",
    "                        y_truelist = []\n",
    "                        y_predlist = []\n",
    "                        for case in test_cases:\n",
    "                            if len(case) >= maximum_prefix:\n",
    "                                x = case[maximum_prefix-1]\n",
    "                                model = training_models['prefix_%s'%(x.prefix_length)][0]\n",
    "                                current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                current_event = pd.Series(current_event).to_frame().T\n",
    "                                y_pred = training_models['prefix_%s'%(x.prefix_length)][0].predict_proba(current_event)\n",
    "                                y_truelist.append(x.true_label)\n",
    "                                y_predlist.append(y_pred)\n",
    "\n",
    "                        prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "                        prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "                        if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "                            cdhappend[maximum_prefix] = casecount\n",
    "    for evaluation_method in ['Accuracy', 'F1', 'ROCAUC']:\n",
    "        prefix = maximum_prefix\n",
    "        update_count =[]\n",
    "        evaluation_list = []\n",
    "        length_list = []\n",
    "        for update in prediction_result[prefix].keys():\n",
    "            y_true = prediction_result[prefix][update]['y_true']\n",
    "            y_pred_prob_ = prediction_result[prefix][update]['y_pred']\n",
    "            prediction_class = training_models['prefix_%s'%(maximum_prefix)][0].classes_\n",
    "            if len(prediction_class) == 1: prediction_class = ['False', 'True']\n",
    "            elif len(prediction_class) == 2: prediction_class = ['False', 'True']\n",
    "            y_pred_prob = []\n",
    "            for i in y_pred_prob_:\n",
    "                y_pred_prob.append({prediction_class[0]: i[0][0], prediction_class[1]: i[0][1]})\n",
    "            y_pred = [max(x, key=x.get) for x in y_pred_prob]\n",
    "            update_count.append(update)\n",
    "\n",
    "            # DEBUGGING\n",
    "            print(\"y_pred\", y_pred)\n",
    "            print(\"y_true\", y_true)\n",
    "            #if maximum_prefix == 15:\n",
    "                # for some reason, the labels in y_pred are \"True\" and \"False\" instead of 1 and 0, while all the previous\n",
    "                # predictions are 1 and 0. This is a fix to make the labels consistent\n",
    "             #   y_pred = [true_to_1[y] for y in y_pred]\n",
    "              #  print(\"y_pred\", y_pred)\n",
    "            # Fit and transform y_true and y_pred\n",
    "            y_true_encoded = le.fit_transform(y_true)\n",
    "            # Ensure y_pred is a numpy array of strings\n",
    "            # Filter out unseen labels\n",
    "            seen_labels = set(le.classes_)\n",
    "            y_pred_filtered = [label for label in y_pred if label in seen_labels]\n",
    "            # ========================\n",
    "            if evaluation_method =='Accuracy':\n",
    "                value = accuracy_score(y_true_encoded, y_pred)\n",
    "                # value = accuracy_score(y_true, y_pred)\n",
    "            elif evaluation_method =='F1':\n",
    "                value = f1_score(y_true, y_pred, average='macro')\n",
    "                # value = f1_score(y_true, y_pred, average='macro')\n",
    "        #             warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            elif evaluation_method =='ROCAUC':\n",
    "                y_pred_prob = np.array([[x['False'], x['True']] for x in y_pred_prob])\n",
    "                try:\n",
    "                    value = roc_auc_score(y_true, y_pred_prob[:, 1])\n",
    "                except:\n",
    "                    value = 0\n",
    "            evaluation_list.append(value)\n",
    "        plt.figure(figsize=(15,8))\n",
    "        plt.plot(update_count, evaluation_list, label = 'Accuracy')\n",
    "        #     plt.plot(update_count, length_list)\n",
    "        if dataset_label == 'bpic15' or dataset_label == 'bpic17':\n",
    "            pass\n",
    "        else:\n",
    "            plt.axvline(cdhappend[prefix], color = 'black', linestyle ='--', label = 'Concept drift')\n",
    "        plt.title(prefix)\n",
    "        plt.xlabel('Model update count')\n",
    "        plt.legend(bbox_to_anchor=(0.65, -0.1), ncol=2)\n",
    "        plt.ylim(0,1)\n",
    "\n",
    "        if os.path.exists('./img/%s/%s/%s'%(dataset_label, classifier, 'Finished cases')) == False:\n",
    "            os.makedirs('./img/%s/%s/%s'%(dataset_label, classifier, 'Finished cases'))\n",
    "        if os.path.exists('./result/%s/%s/%s'%(dataset_label, classifier, 'Finished cases')) == False:\n",
    "            os.makedirs('./result/%s/%s/%s'%(dataset_label, classifier, 'Finished cases'))\n",
    "\n",
    "\n",
    "        with open('./result/%s/%s/Finished cases/prefix_%s_%s update.pkl'%(dataset_label, classifier, prefix, evaluation_method), 'wb') as f:\n",
    "            pkl.dump(prediction_result, f)\n",
    "        plt.savefig('./img/%s/%s/Finished cases/prefix_%s_%s update.png'%(dataset_label, classifier, prefix, evaluation_method), dpi=250)\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a7861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
